---
title: "Multi-Path Selection Algorithm: Group 7"
author: "Nimra Ismail, Savannah Rascon, Lilley Brookshire"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```
# Repository 

The repository that contains the project with our test files and the project for our final package can be found here: <https://github.com/R-4-Data-Science/Final-Project-Group-7/tree/main/FinalProjectNImra>

The repository is named Final_Project_Group_7. 

The project that contains the documentation for our package is in the folder FinalProjectNImra. 

# 5. HTML with Runnable Examples

This section demonstrates runnable examples that illustrate how the **multi-path AIC model selection** algorithm works for both regression families: **Gaussian (linear regression)** and **Binomial (logistic regression)**. These examples use synthetic data to ensure that the package’s functions behave as expected. 

Initially, we show base R–style demonstrations (Sections 5.1 and 5.2) to confirm that the AIC-based model comparisons are functioning properly. Then, in Section 5.3, we demonstrate how to use the actual package workflow to perform a full end-to-end analysis.

---

## 5.1 Linear Regression (Gaussian)

This subsection illustrates the concept of **AIC-based model comparison** using **base R functions** on synthetic continuous data. We simulate Gaussian data with a known linear relationship and compare several simple regression models to demonstrate how AIC penalizes unnecessary complexity while rewarding better fit.

```{r linear-baseR, echo=TRUE}

# Base R Linear Regression Example (Gaussian)

set.seed(1)
n <- 120; p <- 8

# Simulate predictors
X <- matrix(rnorm(n * p), n, p)
colnames(X) <- paste0("x", 1:p)

# Define the true relationship
beta <- c(2, -1.5, 0, 0, 1, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 1)

# Combine into a data frame
df <- data.frame(y, X)

# Fit an intercept-only model
fit0 <- lm(y ~ 1, data = df)
aic0 <- AIC(fit0)

# Fit single-variable models to compare AICs
fits <- lapply(1:p, function(j) lm(as.formula(paste("y ~", colnames(X)[j])), data = df))
aics <- sapply(fits, AIC)

# Display top 5 models with lowest AIC
aic_results <- data.frame(Variable = colnames(X), AIC = aics)
aic_results <- aic_results[order(aic_results$AIC), ]
head(aic_results, 5)
```

The table shows the five predictors with the lowest AIC values from the single-variable linear regression models. Among them, `x1`, `x2`, and `x5` have the smallest AIC scores (535.5, 556.4, and 581.5, respectively), indicating that these predictors provide the best model fit with the least complexity penalty.

Because the synthetic data were generated with a true underlying relationship involving `x1`, `x2`, and `x5`, the AIC-based comparison correctly identifies these as the most informative variables. Predictors such as `x3` and `x7`, while still contributing modestly, show higher AIC values, meaning they offer less improvement in fit relative to the increase in model complexity.

## 5.2 Logistic Regression (Binomial)

In this subsection, we illustrate AIC-based model comparison for a binary classification problem using **base R** logistic regression. We generate binary data from a known logistic model and compare candidate models containing individual predictors. This helps visualize how AIC extends naturally from Gaussian to Binomial models for classification.

```{r logistic-baseR, echo=TRUE}

# Base R Logistic Regression Example (Binomial)

set.seed(2)
n <- 200; p <- 6

# Simulate predictors

Xb <- matrix(rnorm(n * p), n, p)
colnames(Xb) <- paste0("x", 1:p)

# Generate binary outcome based on a logistic function

linpred <- 1.2 * Xb[,1] - 1 * Xb[,2] + 0.8 * Xb[,5]
prob <- 1 / (1 + exp(-linpred))
ybin <- rbinom(n, 1, prob)

# Combine into a data frame

dfb <- data.frame(y = ybin, Xb)

# Fit intercept-only model

fit0 <- glm(y ~ 1, family = binomial(), data = dfb)
aic0 <- AIC(fit0)

# Fit single-variable models

fits1 <- lapply(1:p, function(j) glm(as.formula(paste0("y ~ ", colnames(Xb)[j])),
family = binomial(), data = dfb))
aics1 <- sapply(fits1, AIC)

# Display top 5 models with lowest AIC

aic_results_log <- data.frame(Variable = colnames(Xb), AIC = aics1)
aic_results_log <- aic_results_log[order(aic_results_log$AIC), ]
head(aic_results_log, 5)
```

The table summarizes the AIC values for logistic regression models fitted with each predictor individually. The variables `x1`, `x2`, and `x5` have the lowest AIC scores (241.1, 259.7, and 264.5, respectively), indicating that these predictors provide the best trade-off between model fit and simplicity.

This aligns with the way the synthetic data were generated, where `x1`, `x2`, and `x5` were intentionally set as influential predictors in the logistic model. Their lower AIC values show that these variables most effectively explain the variation in the binary outcome.

## 5.3 Example Usage

This section demonstrates the **complete workflow** using the functions implemented in the `FinalProjectNimra` package.

The purpose of this section is to replace the original placeholder example from the assignment (which used `build_paths()`, `stability()`, and `plausible_models ()`).

We show both linear and logistic regression examples, and this section confirms that the package functions integrate properly and produce interpretable, stable results for both regression families.

```{r install-and-load}
# Install the package (uncomment if not already installed)
# install.packages("remotes")
# remotes::install_github("R-4-Data-Science/Final-Project-Group-7/FinalProjectNImra")

# Load the package
library(FinalProjectNImra)
```

Our chosen defaults for the interal workings of our functions were:
  - K (maximum number of steps) = 5: default of 5 levels for the iterations allows for thorough filtering but avoids long run time.
  - $\epsilon$ (minimum AIC improvement from parent model) = .000001: keeping this smalls allows us to retain models that improve even a small amount from their parent.
  - $\delta$ (AIC tolerance for near ties in children) = 2: this means that only children that perform essentially equally best are retained.
  - $L$ (max number of models kept per level) = 25: this allows for a pool of good models to be kept at each level, but prevents clutter/ long run time.
  - $B$ (number of resamples for stability) = 100: the bootstrap resampling is most effective if the number of resamples is high to improve accuracy, but you must also balance run time, and 100 has worked well.
  - $\Delta$ (AIC tolerance for plausibility) = 2: same as $\delta$, this ensures that only models that are the very best and very close are retained.
  - $\tau$ (minimum average stability) = 0.6: this metric ensures that only variables that appear in the majority of models, and therefore are the most stable, are prioritized.
  
  
## Example 1 – Linear Regression (Gaussian)

We first demonstrate the multi-path AIC workflow on simulated Gaussian data.
The goal is to recover the true underlying predictors while accounting for model uncertainty and stability.

```{r}
set.seed(123)

# Simulate Gaussian data
n <- 120; p <- 8
X <- matrix(rnorm(n * p), n, p)
beta <- c(2, -1.5, 0, 0, 1, rep(0, p - 5))
y <- X %*% beta + rnorm(n, sd = 1)
df <- as.data.frame(cbind(y, X))
colnames(df) <- c("y", paste0("x", 1:p))

# Run the full procedure

# multi-path forward selection
path_forest <- build_paths(
  X = df, 
  response = "y", 
  K = 5,
  epsilon = .000001, 
  delta = 2, 
  L = 25, 
  model_type = NULL
  )

# stability with resampling
stability_scores <- stability(
  X = df, 
  response = "y", 
  B = 100, 
  K = 5, 
  epsilon = 1e-6, 
  delta = 2, 
  L = 25, 
  model_type = NULL
  )

# select plausible models
plausible <- plausible_models(
  full_data_models = path_forest$path_forest$frontiers, 
  stability_df = stability_scores,
  delta = 2,
  tau = 0.6
  )

plausible

AIC_table <- as.matrix(c(plausible[[1]]$aic, plausible[[2]]$aic, plausible[[3]]$aic, plausible[[4]]$aic), nrow = 1, ncol = 4)
rownames(AIC_table) <- c("model 1", "model 2", "model 3", "model 4")
colnames(AIC_table) <- c("Final AIC Values")
AIC_table
boxplot(AIC_table, main = "Final AIC Values")
```

The multi-path AIC procedure identified plausible and stable models that all included `X1`, `X2`, `X5` and `X7`. 

## Example 2 – Logistic Regression (Binomial)

Next, we apply the same approach to synthetic binary data generated from a logistic model.
This demonstrates how the same function can handle classification problems.

```{r}
# Simulate binary logistic data
n <- 200; p <- 6
Xb <- matrix(rnorm(n * p), n, p)
colnames(Xb) <- paste0("x", 1:p)

# Define the true logistic relationship
linpred <- 1.2 * Xb[,1] - 1 * Xb[,2] + 0.8 * Xb[,5]
prob <- 1 / (1 + exp(-linpred))
ybin <- rbinom(n, 1, prob)

# Combine into a data frame
dfb <- as.data.frame(cbind(y = ybin, Xb))

# Run the full procedure

# multi-path forward selection
path_forest_glm <- build_paths(
  X = dfb, 
  response = "y", 
  K = 5,
  epsilon = .000001, 
  delta = 2, 
  L = 25, 
  model_type = NULL
  )

# stability with resampling
stability_scores_glm <- stability(
  X = dfb, 
  response = "y", 
  B = 100, 
  K = 5, 
  epsilon = 1e-6, 
  delta = 2, 
  L = 25, 
  model_type = NULL
  )

# select plausible models
plausible_glm <- plausible_models(
  full_data_models = path_forest_glm$path_forest$frontiers, 
  stability_df = stability_scores_glm,
  delta = 2,
  tau = 0.6
  )

plausible_glm

AIC_table_glm <- as.matrix(c(plausible_glm[[1]]$aic), nrow = 1, ncol = 1)
rownames(AIC_table_glm) <- "model 1"
colnames(AIC_table_glm) <- c("Final AIC Values")
AIC_table_glm
boxplot(AIC_table_glm, main = "Final AIC Values")
```

The multi-path AIC procedure identified one plausible and stable model consisting of predictors `X1`, `X2`, and `X5` as important predictors. These variables correspond to the true predictors used to generate the binary outcome, confirming accurate recovery of the underlying data structure.

**Interpretation:**

These examples demonstrate that the `FinalProjectNImra` package successfully:

+ Explores multiple near-optimal AIC paths

+ Estimates variable stability across bootstrap samples (B = 100).

+ Filters plausible models using AIC tolerance (Δ = 2 by default) and stability threshold (τ = 0.6).

The results verify that both linear and logistic regression families are supported and produce interpretable, stable model selections.

# Vignette

The vignette that is included in the documentation for our package is copied below for convenience.

```{r}
library(FinalProjectNImra)
library(mlbench)
library(dplyr)
library(caret)
set.seed(123)
```

# Introduction

This vignette demonstrates the use of the **FinalProjectNImra** package to perform a complete model selection pipeline based on a **multi-path AIC procedure**.  

The package implements an advanced workflow that:

1. Explores **multiple forward-selection paths** using AIC (`build_paths()`),  
2. Quantifies **variable stability** via bootstrap resampling (`stability()`), and  
3. Filters a **plausible model set** using both AIC and average stability (`plausible_models()`), optionally providing fitted models for prediction.

We illustrate the workflow on the **Breast Cancer Wisconsin** data set for **logistic regression**, including:

- train/test split,  
- multi-path AIC search and stability analysis,  
- plausible model selection with AIC tolerance (Δ) and stability threshold (τ),  
- test-set evaluation using a confusion matrix and performance metrics.

At the end, we summarize how the package functions correspond to the project rubric’s function names and provide additional synthetic examples.
  
# Breast Cancer Example (Main Vignette)

## Data loading and preprocessing

We use the **Breast Cancer Wisconsin** dataset from the **mlbench** package. The goal is to classify tumors as **malignant** (1) vs **benign** (0) based on several cell-nuclei features.

```{r data, echo = FALSE}
library(dplyr)

data("BreastCancer", package = "mlbench")
df <- BreastCancer

# Clean and preprocess
df <- df %>%
  select(-Id) %>%                         # Drop ID column
  filter(complete.cases(.)) %>%          # Remove rows with missing values
  mutate(across(-Class, as.numeric)) %>% # Convert predictors to numeric
  mutate(Class = factor(Class))          # Ensure response is factor

# Rename response to "Diagnosis" for clarity
names(df)[names(df) == "Class"] <- "Diagnosis"

# reset row indices
rownames(df) <- NULL

```

The dataset contains predictors describing characteristics of cell nuclei and a binary target variable indicating whether the tumor is **malignant** or **benign**.

## Train/Test Split

We split the data into **80% training** and **20% testing** subsets to evaluate model performance on unseen data.

```{r split , echo = FALSE}

# split the data
df <- as.data.frame(df)
nrow(df)
split_at_row <- round(nrow(df)*.8, digits = 0)
df_train <- df[1:split_at_row,]
df_test <- df[(split_at_row+1):nrow(df),]
```

This ensures that both subsets maintain class balance for fair model evaluation.

# Multi-Path AIC Search

We now run the **multi-path model selection pipeline** from our package on the training set.  
The parameters used are summarized below:

| Parameter | Description | Value Used |
|------------|--------------|-------------|
| K | Number of paths explored | 5 |
| B | Bootstrap replicates for stability estimation | 100 |
| δ (delta) | AIC threshold for near-optimal models | 2 |
| τ (tau) | Stability cutoff for plausible model inclusion | 0.6 |

> **Note:**

These values provide a balanced trade-off between computational efficiency and exploration depth. The AIC tolerance (`delta = 2`) ensures that near-optimal models are retained, while the stability threshold (`tau = 0.6`) filters predictors that consistently appear in at least 60% of bootstrap samples. Parameters `eps` and `L` are left at default values (`eps = 1e-6`, `L = 25`), which are sufficient for this dataset size.

```{r search , echo = FALSE}
# Run multi-path search on training data
paths_v <- build_paths(
  X = df_train, 
  response = "Diagnosis", 
  K = 5, 
  epsilon = .000001, 
  delta = 2, 
  L = 25, 
  model_type = NULL)

print(paths_v)
```

δ = 2 defines models whose AIC values are within two units of the minimum, meaning they are statistically indistinguishable from the best model.

# Stability Estimation (Bootstrap)

```{r}
# Estimate stability using 100 bootstrap replicates
stability_scores <- stability(
  X = df_train, 
  response = "Diagnosis")

# Print top stable features
print(stability_scores)

```

# Select Plausible Models (AIC + Stability)

```{r}
# Filter plausible models with AIC + Stability thresholds

plausible <- plausible_models(
  full_data_models = paths_v$path_forest$frontiers, 
  stability_df = stability_scores$path_stability)


# Show resulting plausible models
print(plausible)

```

## Evaluate Each Plausible Model (on Test Set)

We now evaluate the **most stable plausible modes** on the held-out test set. The models are fitted on the test data using the predictors identified from the multi-path AIC procedure and then validated on the test data.

After performing the multi-path AIC search and stability estimation, we identify the final plausible models using both AIC and stability-based filtering criteria. This is achieved internally through the function `plausible_models()`, which applies two key thresholds: the **AIC tolerance (Δ)** and the **minimum average stability (τ)**.

Although **Δ** (Delta) is not explicitly specified in our vignette code, it is automatically applied as the argument `delta_AIC = 2` inside the `plausible_models()` function. By default, this threshold retains all models whose AIC values are within **two units of the minimum AIC**, which is the standard guideline for identifying statistically indistinguishable models in terms of fit quality [(Burnham & Anderson, 2002)](https://doi.org/10.1007/b97636). Thus, the plausible model selection process inherently respects this Δ = 2 criterion even without user specification.

Similarly, the stability threshold **τ = 0.6** is used to filter models based on the consistency of variable selection across bootstrap resamples. Variables that appear in at least 60% of resampled models are retained as stable predictors. This threshold balances parsimony and robustness by emphasizing predictors that contribute reliably to model performance across sampling variability [(Meinshausen & Bühlmann, 2010)](https://doi.org/10.1214/09-AOS687).


```{r}
# Calculate confusion metrics

confusion_1 <- confusion_metrics(model = plausible[[1]]$fit, data = df_test)
confusion_2 <- confusion_metrics(model = plausible[[2]]$fit, data = df_test)
confusion_3 <- confusion_metrics(model = plausible[[3]]$fit, data = df_test)
confusion_1
confusion_2
confusion_3
```


***Interpretation:***

The confusion matrix above summarizes the classification performance of the logistic model on the unseen test data, comparing predicted versus actual tumor classes. The confusion matrix shows **excellent classification** performance on the test data.

+ **Accuracy** = `r confusion_1$metrics$Accuracy` → Out of 137 test samples, 135 were correctly classified, yielding an overall accuracy of 98%.

+ **Sensitivity** = `r confusion_1$metrics$Sensitivity` → The model correctly identifies 97% of malignant tumors, demonstrating strong detection ability for positive cases.

+ **Specificity** = `r confusion_1$metrics$Specificity`→ The model correctly identifies 99% of benign tumors, meaning very few benign samples were misclassified as malignant.

+ **FDR** = `r confusion_1$metrics$Precision` → Only about 2% of predicted malignant cases were actually benign, showing that false positives are minimal.

+ **DOR** = 3434 → The extremely high Diagnostic Odds Ratio indicates outstanding discriminative power, confirming the model’s ability to separate malignant and benign cases with high reliability.

Overall, these results confirm that the multi-path AIC–selected model generalizes exceptionally well, achieving high accuracy and balanced performance across sensitivity and specificity. It effectively identifies malignant cases while maintaining very low false-positive and false-negative rates, a crucial feature for reliable medical classification.

# Reproducibility

Ensuring reproducibility is a critical step in any data analysis workflow. The following R session information records package versions and environment details used to generate this vignette. This allows other researchers to replicate the analysis exactly.

```{r session-info}
sessionInfo()
```

# Discussion

The results from the model evaluation demonstrate that the **multi-path AIC procedure** produces a highly accurate and reliable logistic regression model for breast cancer classification. By exploring multiple near-optimal model paths and assessing variable stability through bootstrap resampling, the algorithm identifies predictors that are both **statistically sound** and **biologically meaningful**.



These metrics indicate **excellent generalization performance**.  
The model correctly classified the majority of both malignant and benign cases, with **very few misclassifications**

The high **sensitivity** shows that the model effectively identifies malignant tumors — a crucial property for medical diagnostic models where false negatives can have serious consequences. At the same time, the **specificity** close to 99% ensures that benign cases are rarely misclassified as malignant, minimizing unnecessary concern or follow-up procedures.

Additionally, the earlier **stability analysis** revealed that features such as **Bare.nuclei** and **Cell.size** were consistently selected across bootstrap resamples, reinforcing their importance as **robust and reproducible predictors**. This stability, combined with the high predictive performance, demonstrates that the **multi-path AIC approach** successfully balances **model parsimony**, **interpretability**, and **accuracy**.

**Summary:**  
The multi-path AIC–based selection framework provides a powerful and interpretable solution for identifying reliable predictors of breast cancer malignancy.  

It ensures that the final model is not only statistically optimal (in terms of AIC) but also **clinically meaningful**, achieving strong predictive performance with stable and biologically relevant variables.


# End of Report!!!!!!

# Appendix
## Links for chats with ChatGPT
<https://chatgpt.com/share/693361bd-64c4-800f-b3a3-a2982dedffa5>
<https://chatgpt.com/share/693361d0-dcb8-800f-95d3-fa6da5957fe5>
<https://chatgpt.com/share/693361ea-fd60-800f-894d-a4537e4b80b8>

## Other sourses
Chapter 5,6: <https://smac-group.github.io/ds/section-control.html>
Chapter 25: <https://r4ds.hadley.nz/functions.html>
<https://search.r-project.org/R/refmans/stats/html/lm.html>

