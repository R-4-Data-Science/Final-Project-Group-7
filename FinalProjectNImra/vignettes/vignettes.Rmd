---
title: "Multi-Path AIC Model Selection"
author: "Nimra Ismail"
output:
  html_document:
    css: autumn.css
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: True
    fig_caption: true
    highlight: tango    # Colorful syntax highlighting
vignette: >
  %\VignetteIndexEntry{Multi-Path AIC Model Selection (Breast Cancer Classification)}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(FinalProjectNimra)
library(mlbench)
library(dplyr)
library(caret)
set.seed(123)
```

# Introduction

This vignette demonstrates the use of the **FinalProjectNimra** package to perform a complete model selection pipeline based on a **multi-path AIC procedure**.  

The package implements an advanced workflow that:

1. Explores **multiple forward-selection paths** using AIC (`multi_path_forward()`),  
2. Quantifies **variable stability** via bootstrap resampling (`stability()`), and  
3. Filters a **plausible model set** using both AIC and average stability (`select_plausible_models()`), optionally providing fitted models for prediction.

We illustrate the workflow on the **Breast Cancer Wisconsin** dataset for **logistic regression**, including:

- train/test split,  
- multi-path AIC search and stability analysis,  
- plausible model selection with AIC tolerance (Δ) and stability threshold (τ),  
- test-set evaluation using a confusion matrix and performance metrics.

At the end, we summarize how the package functions correspond to the project rubric’s function names and provide additional synthetic examples.

## Mapping rubric names to package functions

In the project description, the three core functions are called:

- `build_paths()` – multi-path forward selection,  
- `stability()` – resampling-based stability,  
- `plausible_models()` – AIC + stability-based plausible model selection.

In this package, they are implemented as:

- `multi_path_forward()`  ← rubric’s **build_paths()**,  
- `stability()`           ← rubric’s **stability()**,  
- `select_plausible_models()`  ← rubric’s **plausible_models()**.

Additionally, there is a convenience wrapper:

- `multi_path_AIC_procedure()`  
  which runs the full pipeline (search → stability → plausible models) in one call and returns a list containing:
  - `$full_paths` – output of `multi_path_forward()`,  
  - `$stability` – output of `stability()`,  
  - `$plausible_models` – final plausible model table.
  
# Breast Cancer Example (Main Vignette)

## Data loading and preprocessing

We use the **Breast Cancer Wisconsin** dataset from the **mlbench** package. The goal is to classify tumors as **malignant** (1) vs **benign** (0) based on several cell-nuclei features.

```{r data, echo = FALSE}
# Load breast cancer dataset
data("BreastCancer", package = "mlbench")
df <- BreastCancer

# Clean and preprocess
df <- df %>%
  select(-Id) %>%                         # Drop ID column
  filter(complete.cases(.)) %>%          # Remove rows with missing values
  mutate(across(-Class, as.numeric)) %>% # Convert predictors to numeric
  mutate(Class = factor(Class))          # Ensure response is factor

# Rename response to "Diagnosis" for clarity
names(df)[names(df) == "Class"] <- "Diagnosis"

# Confirm structure
str(df)

```

The dataset contains predictors describing characteristics of cell nuclei and a binary target variable indicating whether the tumor is **malignant** or **benign**.

## Train/Test Split

We split the data into **70% training** and **30% testing** subsets to evaluate model performance on unseen data.

```{r split , echo = FALSE}
set.seed(123)
n <- nrow(df)
train_idx <- sample(n, size = round(0.7 * n))
train_data <- df[train_idx, ]
test_data  <- df[-train_idx, ]
```

This ensures that both subsets maintain class balance for fair model evaluation.

# Multi-Path AIC Search

We now run the **multi-path model selection pipeline** from our package on the training set.  
The parameters used are summarized below:

| Parameter | Description | Value Used |
|------------|--------------|-------------|
| K | Number of paths explored | 5 |
| B | Bootstrap replicates for stability estimation | 50 |
| δ (delta) | AIC threshold for near-optimal models | 2 |
| τ (tau) | Stability cutoff for plausible model inclusion | 0.6 |

> **Note:**

These values provide a balanced trade-off between computational efficiency and exploration depth. The AIC tolerance (`delta = 1`) ensures that near-optimal models are retained, while the stability threshold (`tau = 0.6`) filters predictors that consistently appear in at least 60% of bootstrap samples. Parameters `eps` and `L` are left at default values (`eps = 1e-6`, `L = 50`), which are sufficient for this dataset size.

```{r search , echo = FALSE}
# Run multi-path search on training data
res_logistic <- multi_path_forward(
  data = train_data,
  response = "Diagnosis",
  K = 5,
  delta = 2,
  L = 50,
  model_type = "logistic"
)

# Inspect number of models per step
sapply(res_logistic$path_forest, length)

```

δ = 1 defines models whose AIC values are within one unit of the minimum, meaning they are statistically indistinguishable from the best model.

# Stability Estimation (Bootstrap)

```{r}
# Estimate stability using 50 bootstrap replicates
res_stability <- stability(
  data = train_data,
  response = "Diagnosis",
  B = 50,
  K = 5,
  delta = 2,
  L = 50,
  model_type = "logistic"
)

# Print top stable features
print(res_stability)

```
## Stability Visualization

The variable stability plot shows how frequently each predictor appears across bootstrap samples. High stability indicates robust predictors that consistently contribute to good model fits.

```{r}
# Visualize stability scores
library(ggplot2)

# Prepare data frame for plotting
stab_df <- data.frame(
  Predictor = names(res_stability$pi),
  Stability = res_stability$pi
)

stab_df <- stab_df %>%
  arrange(desc(Stability)) %>%
  mutate(Predictor = factor(Predictor, levels = Predictor))  # preserve order

# Plot
ggplot(stab_df, aes(x = Predictor, y = Stability)) +
  geom_col(fill = "steelblue") +
  geom_hline(yintercept = 0.6, linetype = "dashed", color = "red") +
  labs(
    title = "Variable Selection Stability (πⱼ)",
    subtitle = "Dashed line = 0.6 stability threshold",
    y = "Average Stability (πⱼ)",
    x = "Predictor"
  ) +
  coord_flip() +
  theme_minimal()

```

***Interpretation:***

From Figure, Bare.nuclei and Cell.size exhibit the highest stability, each appearing in the majority of bootstrap-selected models. This suggests that these predictors are the most reliable and consistently associated with breast cancer classification outcomes.

Variables such as  Cl.thickness and Cell.shape show moderate stability, implying that they may contribute to the model in some bootstrap samples but are not universally strong predictors.

On the other hand, features like Bl.cromatin, Marg.adhesion, Mitoses, Normal.nucleoli, Id, and Epith.c.size demonstrate low stability, indicating that they are less influential and may represent noise or dataset-specific variability.

Overall, the stability plot highlights Bare.nuclei and Cell.size as the most critical and reproducible predictors across resampled models.

# Select Plausible Models (AIC + Stability)

```{r}
# Filter plausible models with AIC + Stability thresholds
plausible <- select_plausible_models(
  full_data_models = res_logistic,
  stability_obj = res_stability,
  delta_AIC = 2,
  tau = 0.5,
  keep_fits = TRUE
)

# Show resulting plausible models
print(plausible)

# plotting
#ggplot(stab_df, aes(x = Stability)) +
#  geom_histogram(binwidth = 0.05, fill = "purple", color = "white") +
 # geom_vline(xintercept = 0.6, color = "red", linetype = "dashed") +
#  labs(title = "Distribution of Predictor Stabilities", x = "πⱼ", y = "Count")


```


## Evaluate Each Plausible Model (on Test Set)

We now evaluate the **most stable plausible model** on the held-out test set. The model is fitted on the training data using the predictors identified from the multi-path AIC procedure and then validated on the test data.

After performing the multi-path AIC search and stability estimation, we identify the final plausible models using both AIC and stability-based filtering criteria. This is achieved internally through the function `select_plausible_models()`, which applies two key thresholds: the **AIC tolerance (Δ)** and the **minimum average stability (τ)**.

Although **Δ** (Delta) is not explicitly specified in our vignette code, it is automatically applied as the argument `delta_AIC = 2` inside the `select_plausible_models()` function. By default, this threshold retains all models whose AIC values are within **two units of the minimum AIC**, which is the standard guideline for identifying statistically indistinguishable models in terms of fit quality [(Burnham & Anderson, 2002)](https://doi.org/10.1007/b97636). Thus, the plausible model selection process inherently respects this Δ = 2 criterion even without user specification.

Similarly, the stability threshold **τ = 0.6** is used to filter models based on the consistency of variable selection across bootstrap resamples. Variables that appear in at least 60% of resampled models are retained as stable predictors. This threshold balances parsimony and robustness by emphasizing predictors that contribute reliably to model performance across sampling variability [(Meinshausen & Bühlmann, 2010)](https://doi.org/10.1214/09-AOS687).

```{r}
# Apply confusion metrics on each plausible model
evaluation <- lapply(plausible$fit, function(mod) {
  confusion_metrics(mod, data = test_data)
})

# Display metrics for each plausible model
for (i in seq_along(evaluation)) {
  cat("\nModel", i, "-", plausible$variables[i], "\n")
  print(evaluation[[i]]$metrics)
}

```

***Interpretation:***

The confusion matrix above summarizes the classification performance of the logistic model on the unseen test data, comparing predicted versus actual tumor classes. The confusion matrix shows **excellent classification** performance on the test data.

+ **Accuracy** = 0.9608 → Out of 204 test samples, 196 were correctly classified, yielding an overall accuracy of 96.1%.

+ **Sensitivity** = 0.924 → The model correctly identifies 92.4% of malignant tumors, demonstrating strong detection ability for positive cases.

+ **Specificity** = 0.978 → The model correctly identifies 97.8% of benign tumors, meaning very few benign samples were misclassified as malignant.

+ **FDR** = 0.0217 → Only about 2% of predicted malignant cases were actually benign, showing that false positives are minimal.

+ **DOR** = 549 → The extremely high Diagnostic Odds Ratio indicates outstanding discriminative power, confirming the model’s ability to separate malignant and benign cases with high reliability.

Overall, these results confirm that the multi-path AIC–selected model generalizes exceptionally well, achieving high accuracy and balanced performance across sensitivity and specificity. It effectively identifies malignant cases while maintaining very low false-positive and false-negative rates, a crucial feature for reliable medical classification.

# Reproducibility

Ensuring reproducibility is a critical step in any data analysis workflow. The following R session information records package versions and environment details used to generate this vignette. This allows other researchers to replicate the analysis exactly.

```{r session-info}
sessionInfo()
```

# Discussion

The results from the model evaluation demonstrate that the **multi-path AIC procedure** produces a highly accurate and reliable logistic regression model for breast cancer classification. By exploring multiple near-optimal model paths and assessing variable stability through bootstrap resampling, the algorithm identifies predictors that are both **statistically sound** and **biologically meaningful**.

The final model achieved:

- **Accuracy:** 0.96  
- **Sensitivity:** 0.92  
- **Specificity:** 0.98  
- **False Discovery Rate (FDR):** 0.02  
- **Diagnostic Odds Ratio (DOR):** 549  

These metrics indicate **excellent generalization performance**.  
The model correctly classified the majority of both malignant and benign cases, with **very few misclassifications** (only 8 out of 204 total samples).  

The high **sensitivity** shows that the model effectively identifies malignant tumors — a crucial property for medical diagnostic models where false negatives can have serious consequences. At the same time, the **specificity** close to 98% ensures that benign cases are rarely misclassified as malignant, minimizing unnecessary concern or follow-up procedures.

The **DOR value of 549** further confirms the model’s **outstanding discriminative ability** ,values above 100 typically indicate a strong distinction between positive and negative classes, while this result shows near-perfect separation.  

Additionally, the earlier **stability analysis** revealed that features such as **Bare.nuclei** and **Cell.size** were consistently selected across bootstrap resamples, reinforcing their importance as **robust and reproducible predictors**. This stability, combined with the high predictive performance, demonstrates that the **multi-path AIC approach** successfully balances **model parsimony**, **interpretability**, and **accuracy**.

**Summary:**  
The multi-path AIC–based selection framework provides a powerful and interpretable solution for identifying reliable predictors of breast cancer malignancy.  

It ensures that the final model is not only statistically optimal (in terms of AIC) but also **clinically meaningful**, achieving strong predictive performance with stable and biologically relevant variables.
